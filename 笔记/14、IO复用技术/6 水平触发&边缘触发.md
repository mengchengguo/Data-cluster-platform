EPOLLET改为边缘触发

水平触发&边缘触发
===

select和poll采用水平触发

epoll有水平触发和边缘触发两种机制

**水平触发**

- 如果接收缓冲区不为空，表示有数据可读。如果数据没有被读取完，再次调用epoll_wait的时候，读事件一直触发，这是读事件，还有写事件。
- 如果发送缓冲区没有满，表示可以写入数据，只要缓冲区没有写满，再次调用epoll_wait的时候。写事件一直触发。

对于读事件

当有新的客户端，连上来的时候会触发读事件。如果不处理客户端的连接，则事件一直触发

如果客户端有报文过来不接收会怎么样？事件也一直触发。

对于可写事件

也是不断的触发可写事件。

**边缘触发**

- socket加入epoll之后，如果接收缓冲区不为空，触发可读事件，如果有新的数据到达。再次触发可读事件。
- epoll触发可读事件后，不管程序有没有处理可读事件，epoll都不会再触发。只有当新的数据到达的时候，才再次触发可读事件。

服务端初始化完成之后，客户端就可以发送连接请求进行三次握手，如果服务端没有accept这个连接会存放在缓存中

只要缓冲区中有数据，就算这些数据是epoll之前存在的，第一次epoll_wait完成的时候。两种方式都会触发可读事件。如果有新的数据到达，将再次触发可读事件。对于可读事件水平触发和边缘触发不同的是。如果事件没有被处理，水平触发，会不断的通知，而边缘触发只通知一次。

触发事件也叫通知事件，或者叫通知或者叫报告事件

**边缘触发-可写事件**

- socket加入epoll后，如果发送缓冲区不为空，触发可写事件，如果发送缓冲区由**满**变成有**空**时，再次触发可写事件

IO复用需要非阻塞IO
===

需要把监听的socket设为非阻塞

fcntl(sockfd, F_SETFL, O_NONBLOCK);

```
fcntl(sockfd, F_SETFL, O_NONBLOCK);
```

标准的写法把原有的状态保留下来

```
fcntl(sockfd, F_SETFL,fcntl(sockfd, F_SETFL, 0）｜ O_NONBLOCK);
```

- 当数据到达socket缓冲区的时候。可能会因为某些原因被内核丢弃，比如校验和错误。这时候，如果采用了阻塞的IO。唤醒的程序读不到数据accept和recv函数就会阻塞。
- 到达缓冲区的数据可能被别人取走，比如多个进程accept同一个socket引发的惊群现象。只有一个连接到来，但是所有的监听进程都会被唤醒，最终只有一个进程可以accept到这个请求。其他的进程会被阻塞，

​       惊群的问题用加锁的方式来解决

- 在边缘触发模式下，必须使用非阻塞的IO，因为程序中需要用循环读和写。直到这个错误代码的出现，如果使用阻塞的IO很容易被阻塞。

读的方法
===

- 如果接收缓存区中有事件没有处理或者有数据没有读完。
- 水平触发：epoll_wait会重复报告，不必担心遗漏事件。
- 边缘触发：epoll_wait不会重复报告。所以在程序中要用一个循环处理全部的事件。或者读取全部的数据。

写的方法
===

- 想写就直接写，出现EAGAIN就别写了
- 水平触发：epoll_wait会重复报告，如果不想写了可以注销事件
- 边缘触发：epoll_wait不会重复报告，不想写就不写，不必注销事件

水平触发和边缘触发的性能。就好比倚天剑屠龙刀相对比

水平触发与边缘触发的性能差异
===

平时大家使用 epoll 时都知道其事件触发模式有默认的 level-trigger 模式和通过 EPOLLET 启用的 edge-trigger 模式两种。从 epoll 发展历史来看，它刚诞生时只有 edge-trigger 模式，后来因容易产生 race-cond 且不易被开发者理解，又增加了 level-trigger 模式并作为默认处理方式。

二者的差异在于 level-trigger 模式下只要某个 fd 处于 readable/writable 状态，无论什么时候进行 epoll_wait 都会返回该 fd；而 edge-trigger 模式下只有某个 fd 从 unreadable 变为 readable 或从 unwritable 变为 writable 时，epoll_wait 才会返回该 fd。

通常的误区是：level-trigger 模式在 epoll 池中存在大量 fd 时效率要显著低于 edge-trigger 模式。

但从 kernel 代码来看，edge-trigger/level-trigger 模式的处理逻辑几乎完全相同，差别仅在于 level-trigger 模式在 event 发生时不会将其从 ready list 中移除，略为增大了 event 处理过程中 kernel space 中记录数据的大小。

然而，edge-trigger 模式一定要配合 user app 中的 ready list 结构，以便收集已出现 event 的 fd，再通过 round-robin 方式挨个处理，以此避免通信数据量很大时出现忙于处理热点 fd 而导致非热点 fd 饿死的现象。统观 kernel 和 user space，由于 user app 中 ready list 的实现千奇百怪，不一定都经过仔细的推敲优化，因此 edge-trigger 的总内存开销往往还大于 level-trigger 的开销。

一般号称 edge-trigger 模式的优势在于能够减少 epoll 相关系统调用，这话不假，但 user app 里可不是只有 epoll 相关系统调用吧？为了绕过饿死问题，edge-trigger 模式的 user app 要自行进行 read/write 循环处理，这其中增加的系统调用和减少的 epoll 系统调用加起来，有谁能说一定就能明显地快起来呢？

实际上，epoll_wait 的效率是 O(ready fd num) 级别的，因此 edge-trigger 模式的真正优势在于减少了每次 epoll_wait 可能需要返回的 fd 数量，在并发 event 数量极多的情况下能加快 epoll_wait 的处理速度，但别忘了这只是针对 epoll 体系自己而言的提升，与此同时 user app 需要增加复杂的逻辑、花费更多的 cpu/mem 与其配合工作，总体性能收益究竟如何？只有实际测量才知道，无法一概而论。不过，为了降低处理逻辑复杂度，常用的事件处理库大部分都选择了 level-trigger 模式（如 libevent、boost::asio等）

***\*结论：\****

• epoll 的 edge-trigger 和 level-trigger 模式处理逻辑差异极小，性能测试结果表明常规应用场景 中二者性能差异可以忽略。

• 使用 edge-trigger 的 user app 比使用 level-trigger 的逻辑复杂，出错概率更高。

• edge-trigger 和 level-trigger 的性能差异主要在于 epoll_wait 系统调用的处理速度，是否是 user app 的性能瓶颈需要视应用场景而定，不可一概而论